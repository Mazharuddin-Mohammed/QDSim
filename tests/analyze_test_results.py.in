#!/usr/bin/env python3

"""
QDSim Test Results Analysis Tool
Analyzes test results and generates comprehensive reports
"""

import os
import sys
import re
import json
import csv
import argparse
from datetime import datetime
from pathlib import Path
import xml.etree.ElementTree as ET

class TestResultsAnalyzer:
    def __init__(self, log_dir):
        self.log_dir = Path(log_dir)
        self.results = {
            'summary': {},
            'categories': {},
            'performance': {},
            'failures': [],
            'warnings': []
        }
    
    def analyze_logs(self):
        """Analyze all log files in the directory"""
        print(f"Analyzing test results in: {self.log_dir}")
        
        # Find all log files
        log_files = list(self.log_dir.glob("*.log"))
        
        if not log_files:
            print("No log files found!")
            return False
        
        print(f"Found {len(log_files)} log files")
        
        for log_file in log_files:
            self.analyze_log_file(log_file)
        
        # Look for XML test results
        xml_files = list(self.log_dir.glob("*.xml"))
        for xml_file in xml_files:
            self.analyze_xml_results(xml_file)
        
        # Look for CSV performance reports
        csv_files = list(self.log_dir.glob("*_report.csv"))
        for csv_file in csv_files:
            self.analyze_csv_report(csv_file)
        
        return True
    
    def analyze_log_file(self, log_file):
        """Analyze individual log file"""
        category = self.extract_category_from_filename(log_file.name)
        
        try:
            with open(log_file, 'r') as f:
                content = f.read()
            
            # Extract test results
            passed_tests = len(re.findall(r'\[.*PASSED.*\]', content))
            failed_tests = len(re.findall(r'\[.*FAILED.*\]', content))
            skipped_tests = len(re.findall(r'\[.*SKIPPED.*\]', content))
            
            # Extract timing information
            timing_matches = re.findall(r'(\d+)\s*ms', content)
            timings = [int(t) for t in timing_matches if int(t) > 0]
            
            # Extract memory usage
            memory_matches = re.findall(r'(\d+(?:\.\d+)?)\s*(?:MB|GB)', content)
            memory_usage = [float(m) for m in memory_matches]
            
            # Extract error messages
            error_lines = [line.strip() for line in content.split('\n') 
                          if 'error' in line.lower() or 'failed' in line.lower()]
            
            # Extract warnings
            warning_lines = [line.strip() for line in content.split('\n') 
                           if 'warning' in line.lower() or 'skip' in line.lower()]
            
            self.results['categories'][category] = {
                'passed': passed_tests,
                'failed': failed_tests,
                'skipped': skipped_tests,
                'total': passed_tests + failed_tests + skipped_tests,
                'timings_ms': timings,
                'memory_usage': memory_usage,
                'errors': error_lines[:10],  # Limit to first 10 errors
                'warnings': warning_lines[:5]  # Limit to first 5 warnings
            }
            
            # Add to failures if any tests failed
            if failed_tests > 0:
                self.results['failures'].append({
                    'category': category,
                    'failed_count': failed_tests,
                    'log_file': str(log_file),
                    'errors': error_lines[:3]
                })
            
            # Add warnings
            if warning_lines:
                self.results['warnings'].extend([{
                    'category': category,
                    'message': warning
                } for warning in warning_lines[:3]])
            
        except Exception as e:
            print(f"Error analyzing {log_file}: {e}")
    
    def analyze_xml_results(self, xml_file):
        """Analyze XML test results (JUnit format)"""
        try:
            tree = ET.parse(xml_file)
            root = tree.getroot()
            
            # Extract test suite information
            for testsuite in root.findall('.//testsuite'):
                name = testsuite.get('name', 'unknown')
                tests = int(testsuite.get('tests', 0))
                failures = int(testsuite.get('failures', 0))
                errors = int(testsuite.get('errors', 0))
                time = float(testsuite.get('time', 0))
                
                if name not in self.results['categories']:
                    self.results['categories'][name] = {}
                
                self.results['categories'][name].update({
                    'xml_tests': tests,
                    'xml_failures': failures,
                    'xml_errors': errors,
                    'xml_time': time
                })
                
        except Exception as e:
            print(f"Error analyzing XML file {xml_file}: {e}")
    
    def analyze_csv_report(self, csv_file):
        """Analyze CSV performance reports"""
        try:
            with open(csv_file, 'r') as f:
                reader = csv.DictReader(f)
                rows = list(reader)
            
            report_type = self.extract_report_type_from_filename(csv_file.name)
            
            if report_type == 'scalability':
                self.analyze_scalability_report(rows)
            elif report_type == 'bandwidth':
                self.analyze_bandwidth_report(rows)
            
        except Exception as e:
            print(f"Error analyzing CSV file {csv_file}: {e}")
    
    def analyze_scalability_report(self, rows):
        """Analyze scalability performance data"""
        scalability_data = {}
        
        for row in rows:
            test_name = row.get('Test', 'unknown')
            if test_name not in scalability_data:
                scalability_data[test_name] = []
            
            scalability_data[test_name].append({
                'threads': int(row.get('Threads', 1)),
                'speedup': float(row.get('Speedup', 1.0)),
                'efficiency': float(row.get('Efficiency%', 100.0)),
                'time_ms': float(row.get('TimeMS', 0.0))
            })
        
        self.results['performance']['scalability'] = scalability_data
    
    def analyze_bandwidth_report(self, rows):
        """Analyze bandwidth performance data"""
        bandwidth_data = {}
        
        for row in rows:
            test_name = row.get('Test', 'unknown')
            if test_name not in bandwidth_data:
                bandwidth_data[test_name] = []
            
            bandwidth_data[test_name].append({
                'data_size_mb': float(row.get('DataSizeMB', 0)),
                'bandwidth_gb_s': float(row.get('AchievedGBs', 0)),
                'utilization_percent': float(row.get('UtilizationPercent', 0)),
                'memory_type': row.get('MemoryType', 'unknown')
            })
        
        self.results['performance']['bandwidth'] = bandwidth_data
    
    def extract_category_from_filename(self, filename):
        """Extract test category from log filename"""
        if 'parallel_correctness' in filename:
            return 'parallel_correctness'
        elif 'performance' in filename or 'scalability' in filename or 'bandwidth' in filename:
            return 'performance'
        elif 'cross_platform' in filename or 'cuda' in filename or 'mpi' in filename:
            return 'cross_platform'
        elif 'integration' in filename:
            return 'integration'
        elif 'stress' in filename:
            return 'stress'
        elif 'sanitizer' in filename or 'tsan' in filename or 'asan' in filename:
            return 'sanitizer'
        else:
            return 'other'
    
    def extract_report_type_from_filename(self, filename):
        """Extract report type from CSV filename"""
        if 'scalability' in filename:
            return 'scalability'
        elif 'bandwidth' in filename:
            return 'bandwidth'
        else:
            return 'unknown'
    
    def generate_summary(self):
        """Generate test summary"""
        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_skipped = 0
        
        for category, data in self.results['categories'].items():
            total_tests += data.get('total', 0)
            total_passed += data.get('passed', 0)
            total_failed += data.get('failed', 0)
            total_skipped += data.get('skipped', 0)
        
        self.results['summary'] = {
            'total_tests': total_tests,
            'total_passed': total_passed,
            'total_failed': total_failed,
            'total_skipped': total_skipped,
            'success_rate': (total_passed / total_tests * 100) if total_tests > 0 else 0,
            'categories_tested': len(self.results['categories']),
            'categories_with_failures': len(self.results['failures'])
        }
    
    def print_summary_report(self):
        """Print summary report to console"""
        print("\n" + "="*60)
        print("QDSim Test Results Summary")
        print("="*60)
        
        summary = self.results['summary']
        print(f"Total Tests: {summary['total_tests']}")
        print(f"Passed: {summary['total_passed']} ({summary['success_rate']:.1f}%)")
        print(f"Failed: {summary['total_failed']}")
        print(f"Skipped: {summary['total_skipped']}")
        print(f"Categories Tested: {summary['categories_tested']}")
        
        if summary['total_failed'] == 0:
            print("\n🎉 ALL TESTS PASSED!")
        else:
            print(f"\n❌ {summary['total_failed']} tests failed")
        
        print("\nCategory Breakdown:")
        print("-" * 40)
        for category, data in self.results['categories'].items():
            status = "✓" if data.get('failed', 0) == 0 else "✗"
            print(f"{status} {category}: {data.get('passed', 0)}/{data.get('total', 0)} passed")
        
        # Performance summary
        if self.results['performance']:
            print("\nPerformance Highlights:")
            print("-" * 40)
            
            if 'scalability' in self.results['performance']:
                scalability = self.results['performance']['scalability']
                for test_name, data in scalability.items():
                    if data:
                        max_speedup = max(d['speedup'] for d in data)
                        print(f"Max speedup ({test_name}): {max_speedup:.2f}x")
            
            if 'bandwidth' in self.results['performance']:
                bandwidth = self.results['performance']['bandwidth']
                for test_name, data in bandwidth.items():
                    if data:
                        max_bandwidth = max(d['bandwidth_gb_s'] for d in data)
                        print(f"Max bandwidth ({test_name}): {max_bandwidth:.1f} GB/s")
        
        # Failures summary
        if self.results['failures']:
            print(f"\nFailure Details:")
            print("-" * 40)
            for failure in self.results['failures'][:5]:  # Show first 5 failures
                print(f"Category: {failure['category']}")
                print(f"Failed tests: {failure['failed_count']}")
                if failure['errors']:
                    print(f"Sample error: {failure['errors'][0][:100]}...")
                print()
        
        # Warnings summary
        if self.results['warnings']:
            print(f"\nWarnings ({len(self.results['warnings'])}):")
            print("-" * 40)
            for warning in self.results['warnings'][:3]:  # Show first 3 warnings
                print(f"{warning['category']}: {warning['message'][:80]}...")
    
    def save_json_report(self, output_file):
        """Save detailed results to JSON file"""
        try:
            with open(output_file, 'w') as f:
                json.dump(self.results, f, indent=2, default=str)
            print(f"\nDetailed results saved to: {output_file}")
        except Exception as e:
            print(f"Error saving JSON report: {e}")
    
    def save_html_report(self, output_file):
        """Save HTML report"""
        try:
            html_content = self.generate_html_report()
            with open(output_file, 'w') as f:
                f.write(html_content)
            print(f"HTML report saved to: {output_file}")
        except Exception as e:
            print(f"Error saving HTML report: {e}")
    
    def generate_html_report(self):
        """Generate HTML report content"""
        summary = self.results['summary']
        
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>QDSim Test Results</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ display: flex; gap: 20px; margin: 20px 0; }}
        .metric {{ background-color: #e8f4f8; padding: 15px; border-radius: 5px; text-align: center; }}
        .category {{ margin: 10px 0; padding: 10px; border-left: 4px solid #ccc; }}
        .passed {{ border-left-color: #4CAF50; }}
        .failed {{ border-left-color: #f44336; }}
        .performance {{ background-color: #f9f9f9; padding: 15px; margin: 10px 0; }}
        table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>QDSim Test Results Report</h1>
        <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="summary">
        <div class="metric">
            <h3>{summary['total_tests']}</h3>
            <p>Total Tests</p>
        </div>
        <div class="metric">
            <h3>{summary['total_passed']}</h3>
            <p>Passed</p>
        </div>
        <div class="metric">
            <h3>{summary['total_failed']}</h3>
            <p>Failed</p>
        </div>
        <div class="metric">
            <h3>{summary['success_rate']:.1f}%</h3>
            <p>Success Rate</p>
        </div>
    </div>
    
    <h2>Test Categories</h2>
"""
        
        for category, data in self.results['categories'].items():
            status_class = "passed" if data.get('failed', 0) == 0 else "failed"
            html += f"""
    <div class="category {status_class}">
        <h3>{category.replace('_', ' ').title()}</h3>
        <p>Passed: {data.get('passed', 0)}, Failed: {data.get('failed', 0)}, Skipped: {data.get('skipped', 0)}</p>
    </div>
"""
        
        html += """
</body>
</html>
"""
        return html

def main():
    parser = argparse.ArgumentParser(description='Analyze QDSim test results')
    parser.add_argument('log_dir', help='Directory containing test log files')
    parser.add_argument('--json', help='Output JSON report file')
    parser.add_argument('--html', help='Output HTML report file')
    parser.add_argument('--quiet', action='store_true', help='Suppress console output')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.log_dir):
        print(f"Error: Log directory '{args.log_dir}' does not exist")
        return 1
    
    analyzer = TestResultsAnalyzer(args.log_dir)
    
    if not analyzer.analyze_logs():
        print("No test results found to analyze")
        return 1
    
    analyzer.generate_summary()
    
    if not args.quiet:
        analyzer.print_summary_report()
    
    if args.json:
        analyzer.save_json_report(args.json)
    
    if args.html:
        analyzer.save_html_report(args.html)
    
    # Return exit code based on test results
    return 1 if analyzer.results['summary']['total_failed'] > 0 else 0

if __name__ == '__main__':
    sys.exit(main())
